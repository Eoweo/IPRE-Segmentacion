{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">CNN Segmentation - Neural Network</h1>\n",
    "<h3 style=\"text-align: center;\">Carlos Moreno</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD, Adam\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4070 SUPER\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available(): \n",
    "    dev = \"cuda\" \n",
    "else: \n",
    "    dev = \"cpu\" \n",
    "device = torch.device(\"cpu\")\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "->Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainDataset(Dataset):\n",
    "    def __init__(self, filepath):\n",
    "        self.x, self.y = torch.load(filepath) #Upload ·· x - Number, y - Labels\n",
    "        self.x = self.x / 255. #Normalizamos\n",
    "        self.y = F.one_hot(self.y, num_classes=10).to(float) # Enconder para clasificación por vectores\n",
    "    def __len__(self):  \n",
    "        return self.x.shape[0]\n",
    "    def __getitem__(self, ix): \n",
    "        return self.x[ix], self.y[ix] # i-ésimo numero, label iésima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeuralNet(nn.Module):\n",
    "    def __init__(self, num_classes = 10):    \n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(16 * 7 * 7, num_classes)\n",
    "        self.R = nn.ReLU()\n",
    "    def forward(self,x):\n",
    "        x = self.R(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.R(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clase Dataset (Inicializa los datos y los reordena)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = MainDataset('..\\\\Database\\\\MNIST\\\\processed\\\\training.pt')\n",
    "test_ds = MainDataset('..\\\\Database\\\\MNIST\\\\processed\\\\test.pt')\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=5, shuffle=True) # se divide el dataset en batch de a 5\n",
    "test_dl = DataLoader(test_ds, batch_size=5, shuffle=True) # se divide el dataset en batch de a 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dl, f, n_epochs=20, device = \"cpu\"):\n",
    "    # Optimization\n",
    "    opt =optimizer = Adam(f.parameters(), lr=3e-4) # karpathy's constant\n",
    "    L = nn.CrossEntropyLoss()\n",
    "\t\n",
    "    # Train model\n",
    "    losses = []\n",
    "    epochs = []\n",
    "    accuracy = []\n",
    "    for epoch in range(n_epochs):\n",
    "        N = len(dl)\n",
    "            \n",
    "        for i, (x, y) in enumerate(dl):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            # Update the weights of the network\n",
    "            opt.zero_grad() \n",
    "            loss_value = L(f(x), y) \n",
    "            loss_value.backward() \n",
    "            opt.step() \n",
    "            # Store training data\n",
    "            epochs.append(epoch+i/N)\n",
    "            losses.append(loss_value.item())\n",
    "            \n",
    "        print(f\"Epoch {epoch + 1} is done; loss = { sum (losses[epoch*i: (epoch + 1)*i]) / 12000 }\")\n",
    "        accuracy.append(CheckAccuracy(train_dl, f, \"Train Data\")[0])\n",
    "        \n",
    "    return np.array(epochs), np.array(losses), accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
